#   LangChain

##  ▪️ Definition
LangChain is a *FrameWork* built round [LMs(**L**arge **L**guage **M**odels)](https://www.pinecone.io/learn/openai-gen-qa/) can be used for chatbots, Generative Question-Ansering, summarization ans more.

**Core Idea**: *"Chain "* different components together to create more advanced use cases around LLMS.
**Components**: 
  + **Prompt templates**: Like "chatbot" style templates, ELI5 question-answering, etc. Templates for different types of Prompts (Simple text, Dialog Prompt, Code an Instruction Prompt, all are in string form)
  + **LLMs**: Large Language models like GPT-3, BLOOM, etc
  + **Agents**: use LLMs to decide wht actions should be taken. Tools like web search or calculators are all avilable. They are all packed in a logical loop of operations.
  + **Memory**: Short-/Long-term memory
## ▪️ [Start](/Get_Started/Introduction.ipynb)
### Creating a simple templates in langchain
For the basic setting:
<pre>
import os
from langchain import PromptTemplate
from langchain.llms import OpenAI
os.environ['OPENAI_API_KEY'] = "sk-.."
llm = OpenAI(temperature = 0.9)
</pre>

where: 
+ **temperature** parameter is to cntrol the randomness of the output generated by the OpenAI language model. It will influence the divercity of the model's responses.
  * High Temperature: encourages the model to produce more varied and unpredictable responses. This can lead to more creative outputs but may influence more errors or nonsensitive answers
  * Low Temperature: makes model more conservative and focused. It generates responses that are more deterministic and likely to be contextually appropriate. However, this can also result in more repetitive or safe responses.
### Asking Multiple Questions
Mainly two approaches:
  + Iterate through all questions using the *generate* method, answering them one at a time.
  + Place all questions into a single prompt for the LLM

### OpenAI LLMs
The OpenAI endpoints in Langchain connect to OpenAI directly or via Azure.
## ▪️ Prompt Templates: Manage prompts for LLMs
LLM combines models for classification, named entity recognition(NER), question-answering(QA) and many other tasks. The LangChain library recognizes prompts and hs built an entire set of objects for them. 
### Prompt Engineering
***Prompt*** is a composation of: instructions, context, user input and output indicator.
  + **Instruction**: tell the model what to do, how to use external information, how to act with inputs and to construct outputs.
  + **Context/External Information**: an additional source of knowledge for the model. It can be mannually inserted or pulled in via other means like APIs, Calculations, etc.
  + **User Input**: basicall the question you want to ask or ask the model to do somethings
  + **Output Indicator**: marks the beginning of the to-be-generated text.

### Prompt Templates
To make constructing prompts with dynamic inputs easier, we can feed all the information into a prompt via template. PromptTemplates are responsible for constructing a prompt value. It can do things like formating, example selection and more. 
*PromptTemplate* can train our model with instructions and context of the prompt and leave the question with {query}. With the help of *format* method, we can pass a *query* to the template. There are essencially two distinct prompt templates available:
+ String Prompt Templates
    provides a simple prompt in string formt.
    Two Requirements:
      * *input_variable*
      * *a format method*, which takes in keyword arguments corresponding to the expected input_variables and returns the formatted prompt.
+ Chat Prompt Templates
    produces a more structured prompt to be used with a chat API.


### Few Shot Prompt Templates
With *FewShotPromptTemplate()*, we need parameters like: 
+ *example*: to give examples of how you want the agent to answer you, formally, friendly...
+ *example_prompt*: a prompt with is of form of the examples, e.g. it can contains input_variables of "query"--user's question and "answer"--what we expected it to answer.
+ *prefix*: basically is our instructions
+ *suffix*: user input and output indicator
+ *input_variables*: the same as prompt template
+ *example_separator*: how the examples are separated


## ▪️ Building Custom Tools for LLM Agents

**Agents** are one of the most powerful and fascination approaches to using LLMs. Using Agents allows us to give LLMs access to tools. 
### Building Tools
Tools are objects that consume some input and return output, both in form of *string*. For LangChain requires to recognize an object as a valid tool, we need two attributes: *name* and *description* parameters.
  + *description*:  a natural language description of the tool. Based on that,LLM can decide whether it needs to be use it. **COULD BE**: what they do, when to use and when not to use them 

**There are mainly four types of Agents:**
+ ***zero-shot-react-description***: This agent uses the ReAct framework to determine which tool to use based solely on the tool’s description. Any number of tools can be provided. This agent requires that a description is provided for each tool.
+ ***react-docstore***: This agent uses the ReAct framework to interact with a docstore. Two tools must be provided(they must be named exactly as so): 
    * a *Search* tool: search for a document
    * a *Lookup* tool: lookup a term in the most recently found document. 
+ ***self-ask-with-search***: This agent utilizes a single tool that should be named *Intermediate* Answer. This tool should be able to lookup factual answers to questions. This agent is equivalent to the original self ask with search paper, where a Google search API was provided as the tool.
+ ***conversational-react-description***: This agent is designed to be used in conversational settings. The prompt is designed to make the agent helpful and conversational. It uses the ReAct framework to decide which tool to use, and uses memory to remember the previous conversation interactions.
To buildcustom tools, we need first to initilize the **LLM** and **conversational memory** (for conversational agent). After that it also requires a list of tools, which are to be used. When all initial settings are done, we then need to create  agent using *langchain.agents.initilize_agent()*. With Parameters:
+ agent = *agent_type*
+ tools = *tool set we prepared*
+ llm = *llm*
+ verbose = True(?)
+ memory = *the conversational memory we created*

❗Take a look at the optput of the **AgentExecutor Chain**, it jumped straight to the **Final Answer** action, which means the agent decided *not* to use the tool. So we need more information to let it use the tools instead of 'overconfidence'. That's why we need to create a new prompt and replace the old one (if there has a method to add instead of create a new one?).  

