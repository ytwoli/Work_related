{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f926939-7432-48bd-8c83-47750a4392bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.agents import initialize_agent, AgentExecutor\n",
    "from langchain.agents import load_tools, AgentType\n",
    "from typing import Optional, Type\n",
    "from langchain.chains import LLMChain\n",
    "import os\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "open_AI_Key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0de3e67-c0b1-4366-bf1d-ec14c1576d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feast is an open source project that collects anonymized error reporting and usage statistics. To opt out or learn more see https://docs.feast.dev/reference/usage\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'user_transaction_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtecton\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfeast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureService\n\u001b[0;32m      4\u001b[0m user_transaction_metrics \u001b[38;5;241m=\u001b[39m FeatureService(\n\u001b[0;32m      5\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_transaction_metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m----> 6\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[43muser_transaction_counts\u001b[49m])\n\u001b[0;32m      8\u001b[0m workspace \u001b[38;5;241m=\u001b[39m tecton\u001b[38;5;241m.\u001b[39mget_workspace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprod\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m feature_service \u001b[38;5;241m=\u001b[39m workshop\u001b[38;5;241m.\u001b[39mget_feature_service(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_transaction_metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'user_transaction_counts' is not defined"
     ]
    }
   ],
   "source": [
    "import tecton\n",
    "from feast import FeatureService\n",
    "\n",
    "user_transaction_metrics = FeatureService(\n",
    "    name = \"user_transaction_metrics\",\n",
    "    features = [user_transaction_counts])\n",
    "\n",
    "workspace = tecton.get_workspace(\"prod\")\n",
    "feature_service = workshop.get_feature_service(\"user_transaction_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f028adc-900b-41a1-95ae-db64f6725bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "65115ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class search(BaseModel):\n",
    "    name = \"search_information\"\n",
    "    description = \"\"\"\n",
    "                Use this when you want to find values of power/voltage/current\n",
    "                of a device like battery, modul or panel. \n",
    "                It usually has direction of the current with in/out. \n",
    "                For voltage or power the direction is denoted as None. \n",
    "                it contains the following parameter\n",
    "                {'device': '','information','direction'}\n",
    "                \"\"\"\n",
    "    def _run(infor):\n",
    "        device = infor.get(\"devices\")\n",
    "        information = infor.get(\"information\")\n",
    "        direction = infor.get(\"direction\")\n",
    "        value = 476\n",
    "        return (\"The %s of %s is %s.\" % (informtion, device, value))\n",
    "    \n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support search\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de48ea5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'information': 'power', 'devices': 'battery', 'directions': 'None'}\n",
      "{'deivce': 'battery', 'information': 'power', 'direction': None, 'value': 4566}\n",
      "\n",
      "        Answer the question based on the result given\n",
      "        question: {question},\n",
      "        result:{result}\n",
      "        If the question cannot be answerd using the information provided,\n",
      "        answer with \"I don't understand what you mean. Can you be more specifiy?\"\n",
      "        \n",
      "        Example: \n",
      "        question: Can you tell me the power of my battery?\n",
      "        result: {'deivce': 'battery', 'information': 'power', 'direction': None, 'value': 4566}\n",
      "        answer: The power of your battery is 4566\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't understand what you mean. Can you be more specific?\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def extract_information(sentence):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    information = \"\"\n",
    "    devices = \"\"\n",
    "    directions = \"None\"\n",
    "\n",
    "    # Iterate through the tokens in the sentence\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            # Map nouns to predefined values for information and devices\n",
    "            if token.text.lower() in [\"power\", \"voltage\", \"current\"]:\n",
    "                information = token.text.lower()\n",
    "            elif token.text.lower() in [\"panel\", \"module\", \"battery\"]:\n",
    "                devices = token.text.lower()\n",
    "        elif token.pos_ == \"ADJ\":\n",
    "            # Map verbs to predefined values for directions\n",
    "            if token.text.lower() in [\"in\", \"out\"]:\n",
    "                directions = token.text.lower()\n",
    "\n",
    "    # Create the dictionary\n",
    "    data = {\n",
    "        \"information\": information,\n",
    "        \"devices\": devices,\n",
    "        \"directions\": directions\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "question = \"Can you find the in power of the battery?\"\n",
    "out = extract_information(question)\n",
    "print(out)\n",
    "print(data(out))\n",
    "\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "        memory_key='chat_history',\n",
    "        k=5,\n",
    "        return_messages=True\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "        Answer the question based on the result given\n",
    "        question: {question},\n",
    "        result:{result}\n",
    "        If the question cannot be answerd using the information provided,\n",
    "        answer with \"I don't understand what you mean. Can you be more specifiy?\"\n",
    "        \n",
    "        Example: \n",
    "        question: Can you tell me the power of my battery?\n",
    "        result: {'deivce': 'battery', 'information': 'power', 'direction': None, 'value': 4566}\n",
    "        answer: The power of your battery is 4566\n",
    "        \"\"\"\n",
    "class SearchPromptTemplate(StringPromptTemplate, BaseModel):\n",
    "    \"\"\" A custom prompt template that takes in the question and \n",
    "    form it into a json format as input, \n",
    "    and formats the prompt template to provide the correspondent results. \"\"\"\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        \n",
    "        searching = extract_information(kwargs[\"question\"])\n",
    "        result = data(searching)\n",
    "\n",
    "        # Generate the prompt to be sent to the language model\n",
    "        prompt = template\n",
    "        return prompt\n",
    "    \n",
    "    def _prompt_type(self):\n",
    "        return \"search informtion\"\n",
    "     \n",
    "llm = ChatOpenAI(\n",
    "        openai_api_key=open_AI_Key,\n",
    "        temperature=0,\n",
    "        model_name='gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "prompt1 = SearchPromptTemplate(\n",
    "    input_variables = ['question']\n",
    ")\n",
    "\n",
    "\n",
    "print(prompt1.format(question=question))\n",
    "chain = LLMChain(llm=llm, prompt=prompt1)\n",
    "chain.run(question =question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5a8541f-546d-4508-8a9f-7bc9140fc959",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Act as a Sunsniffer chatbot, \n",
      "        Sunsniffer is a solar analytics and installation company, \n",
      "        answer the question accordingly, and for the installation.\n",
      "        Answer the question based on the context below. \n",
      "        If the question cannot be answerd using the information provided,\n",
      "        answer with \"I don't understand what you mean. Can you be more specifiy?\"\n",
      "        \n",
      "        Context:\n",
      "        \n",
      "        Your goal is:\n",
      "        - through the {question} to form a json format \n",
      "        - using the {tool} to get the information and generate them in English\n",
      "        \n",
      "        Here are some examples:\n",
      "        question: Can I have the in power of my battery?\n",
      "        json format: {\"information\": \"power\",\n",
      "                        \"devices\": \"battery\",\n",
      "                        \"direction\": in}\n",
      "        tool: data()\n",
      "        answer: Your battery has a in-direction power with value 5672 Watt\n",
      "        \n",
      "        In \"devices\", it can contain:\n",
      "        - battery\n",
      "        - panel\n",
      "        - modul\n",
      "        \n",
      "        In \"directions\", it can contain: \n",
      "        - in\n",
      "        - out \n",
      "        - None for not mentioned.\n",
      "        In \"information\", it can contain:\n",
      "        - power\n",
      "        - voltage,\n",
      "        - current\n",
      "           \n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I\\'m sorry, but I am a chatbot for Sunsniffer, a solar analytics and installation company. I don\\'t understand what you mean by \"question\", \"json format\", and \"tool\". Can you please provide me with a specific question related to solar analytics or installation?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "template = \"\"\"Answer the following questions as best you can, \n",
    "but speaking as a secretary. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "Tool =[search()]\n",
    "class SearchPromptTemplate(StringPromptTemplate, BaseModel):\n",
    "    template: str\n",
    "    tools: List[Tool]\n",
    "    def format(self, **kwargs) -> str:\n",
    "        \n",
    "\n",
    "        # Generate the prompt to be sent to the language model\n",
    "        prompt = template\n",
    "        return prompt\n",
    "    \n",
    "    def _prompt_type(self):\n",
    "        return \"search informtion\"\n",
    "     \n",
    "llm = ChatOpenAI(\n",
    "        openai_api_key=open_AI_Key,\n",
    "        temperature=0,\n",
    "        model_name='gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "prompt1 = SearchPromptTemplate(\n",
    "    input_variables = ['question']\n",
    ")\n",
    "\n",
    "\n",
    "question = \"Can you find the in power of the battery?\"\n",
    "\n",
    "print(prompt1.format(question=question))\n",
    "chain = LLMChain(llm=llm, prompt=prompt1)\n",
    "chain.run(question =question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e54b018-d615-482f-9f04-401ef459f465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain/llms/openai.py:169: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/llms/openai.py:692: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument `prompt` is expected to be a string. Instead found <class 'langchain.prompts.prompt.PromptTemplate'>. If you want to run the LLM on multiple prompts, use `generate` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[1;32m      2\u001b[0m         openai_api_key\u001b[38;5;241m=\u001b[39mopen_AI_Key,\n\u001b[1;32m      3\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      4\u001b[0m         model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m conversational_memory \u001b[38;5;241m=\u001b[39m ConversationBufferWindowMemory(\n\u001b[1;32m      8\u001b[0m         memory_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m         k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     10\u001b[0m         return_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m agent \u001b[38;5;241m=\u001b[39m initialize_agent(\n\u001b[1;32m     14\u001b[0m     agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     memory\u001b[38;5;241m=\u001b[39mconversational_memory\n\u001b[1;32m     21\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:291\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check Cache and run the LLM on the given prompt and input.\"\"\"\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m     )\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate([prompt], stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    300\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Argument `prompt` is expected to be a string. Instead found <class 'langchain.prompts.prompt.PromptTemplate'>. If you want to run the LLM on multiple prompts, use `generate` instead."
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "        openai_api_key=open_AI_Key,\n",
    "        temperature=0,\n",
    "        model_name='gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "        memory_key='chat_history',\n",
    "        k=5,\n",
    "        return_messages=True\n",
    ")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent='STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION',\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory=conversational_memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe3183a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Given the function name and source code, generate an English language explanation of the function.\n",
      "        Function Name: get_source_code\n",
      "        Source Code:\n",
      "        def get_source_code(function_name):\n",
      "    # Get the source code of the function\n",
      "    return inspect.getsource(function_name)\n",
      "\n",
      "        Explanation:\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The function \"get_source_code\" takes in a parameter called \"function_name\" and returns the source code of that function. This is achieved by using the \"inspect\" module\\'s \"getsource\" function, which retrieves the source code of the specified function. The returned source code can be used for various purposes, such as debugging or analyzing the function\\'s implementation.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a custom prompt template that takes in the function name as input \n",
    "#and formats the prompt to provide the source code of the function\n",
    "import inspect\n",
    "\n",
    "def get_source_code(function_name):\n",
    "    # Get the source code of the function\n",
    "    return inspect.getsource(function_name)\n",
    "\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        openai_api_key=open_AI_Key,\n",
    "        temperature=0,\n",
    "        model_name='gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "class FunctionExplainerPromptTemplate(StringPromptTemplate, BaseModel):\n",
    "    \"\"\" A custom prompt template that takes in the function name as input, \n",
    "    and formats the prompt template to provide the source code of the function. \"\"\"\n",
    "\n",
    "    @validator(\"input_variables\")\n",
    "    def validate_input_variables(cls, v):\n",
    "        \"\"\" Validate that the input variables are correct. \"\"\"\n",
    "        if len(v) != 1 or \"function_name\" not in v:\n",
    "            raise ValueError(\"function_name must be the only input_variable.\")\n",
    "        return v\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the source code of the function\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        # Generate the prompt to be sent to the language model\n",
    "        prompt = f\"\"\"\n",
    "        Given the function name and source code, generate an English language explanation of the function.\n",
    "        Function Name: {kwargs[\"function_name\"].__name__}\n",
    "        Source Code:\n",
    "        {source_code}\n",
    "        Explanation:\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def _prompt_type(self):\n",
    "        return \"function-explainer\"\n",
    "    \n",
    "fn_explainer = FunctionExplainerPromptTemplate(input_variables=[\"function_name\"])\n",
    "\n",
    "# Generate a prompt for the function \"get_source_code\"\n",
    "prompt = fn_explainer.format(function_name=get_source_code)\n",
    "print(prompt)\n",
    "chain = LLMChain(llm=llm, prompt=fn_explainer)\n",
    "chain.run(get_source_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa5d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156035cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
